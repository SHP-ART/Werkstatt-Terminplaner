# Lokale KI mit Hardware-Beschleuniger

## Ãœbersicht

Dieses Dokument beschreibt, wie die lokale KI des Werkstatt-Terminplaners mit dedizierter Hardware beschleunigt werden kann - von der gÃ¼nstigen Mini-PC-LÃ¶sung bis zum leistungsstarken System mit AI-Beschleuniger.

---

## Hardware-Optionen im Vergleich

| Option | Leistung | Preis | Installation | Empfehlung |
|--------|----------|-------|--------------|------------|
| ğŸ¥‡ **Intel N100 Mini-PC** | CPU (AVX2) | ~130â‚¬ | Fertig | **Beste Wahl** |
| ğŸ¥ˆ **Google Coral USB** | 4 TOPS | ~60â‚¬ | USB einstecken | Budget |
| ğŸ¥‰ **BeagleBone AI-64** | 8 TOPS | ~185â‚¬ | Standalone | Industrie |
| **RPi 5 + Hailo-8** | 26 TOPS | ~240â‚¬ | Standalone | Overkill |

---

## Option 1: Intel N100/N95 Mini-PC (EMPFOHLEN)

### Warum Intel N100?

FÃ¼r die Text-basierten KI-Modelle (ZeitschÃ¤tzung, Arbeiten-VorschlÃ¤ge) ist **kein NPU/TPU nÃ¶tig**. Ein moderner x86-Prozessor mit AVX2-Instruktionen ist schneller und einfacher als ARM + Beschleuniger.

### Vergleich Intel vs Raspberry Pi

| Eigenschaft | Intel N100 | Intel N95 | Raspberry Pi 5 |
|-------------|------------|-----------|----------------|
| **Kerne** | 4 (E-Cores) | 4 (E-Cores) | 4 (Cortex-A76) |
| **Takt** | bis 3.4 GHz | bis 3.4 GHz | 2.4 GHz |
| **RAM** | bis 16 GB DDR5 | bis 16 GB DDR4 | 4/8 GB |
| **AVX2** | Ja | Ja | Nein |
| **TDP** | 6W | 6W | 5W |

### Performance fÃ¼r KI-Modelle

| Aufgabe | RPi 5 | Intel N95 | Intel N100 |
|---------|-------|-----------|------------|
| **Text-Embedding (MiniLM)** | ~100 ms | ~45 ms | ~40 ms |
| **ZeitschÃ¤tzung** | ~2 ms | <1 ms | <1 ms |
| **Gesamt pro Anfrage** | ~110 ms | ~50 ms | ~45 ms |

**Intel ist ~2x schneller** dank AVX2-SIMD-Instruktionen!

### Vorteile

- **Schneller** - 2x schneller als RPi 5 ohne Zusatzhardware
- **Einfacher** - x86 = alle Python-Pakete laufen direkt
- **Mehr RAM** - bis 16 GB mÃ¶glich
- **Leise** - viele Mini-PCs sind passiv gekÃ¼hlt
- **SSD-Support** - SATA/NVMe fÃ¼r schnellen Speicher

### Architektur

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Lokales Netzwerk                         â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Werkstatt-Server   â”‚         â”‚    Intel N100 Mini-PC    â”‚ â”‚
â”‚  â”‚   (Backend + DB)     â”‚  HTTP   â”‚    (passiv gekÃ¼hlt)      â”‚ â”‚
â”‚  â”‚                      â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚                          â”‚ â”‚
â”‚  â”‚  - Node.js Backend   â”‚  REST   â”‚  - KI-Service (Python)   â”‚ â”‚
â”‚  â”‚  - SQLite DB         â”‚   API   â”‚  - AVX2 Beschleunigung   â”‚ â”‚
â”‚  â”‚  - Frontend          â”‚         â”‚  - mDNS Discovery        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Empfohlene GerÃ¤te

| GerÃ¤t | Preis | Besonderheiten |
|-------|-------|----------------|
| **Trigkey G4** | ~120â‚¬ | N95, 8GB, 256GB SSD |
| **Beelink Mini S12** | ~130â‚¬ | N95, 8GB, 256GB SSD |
| **Minisforum UN100C** | ~150â‚¬ | N100, 8GB, 256GB SSD |
| **GMKtec G3** | ~140â‚¬ | N100, 8GB, 512GB SSD |

### Installation

```bash
# 1. Ubuntu Server 22.04/24.04 oder Debian 12 installieren

# 2. Python-Umgebung einrichten
sudo apt update
sudo apt install python3-pip python3-venv
python3 -m venv ~/werkstatt-ki
source ~/werkstatt-ki/bin/activate

# 3. KI-Pakete installieren (AVX2 wird automatisch genutzt)
pip install fastapi uvicorn numpy
pip install sentence-transformers  # MiniLM fÃ¼r Text-Embeddings
pip install scikit-learn           # FÃ¼r Regression/Klassifikation
pip install onnxruntime            # Optimierte Inferenz

# 4. mDNS fÃ¼r automatische Erkennung
sudo apt install avahi-daemon
pip install zeroconf

# 5. KI-Service starten
python app/main.py
```

### Warum kein NPU/TPU nÃ¶tig?

Die KI-Modelle fÃ¼r den Werkstatt-Terminplaner sind klein:

| Modell | GrÃ¶ÃŸe | Operationen/Anfrage |
|--------|-------|---------------------|
| MiniLM (Embeddings) | ~30 MB | ~20 Mio |
| Regression (Zeit) | ~2 MB | ~10.000 |
| Klassifikation | ~5 MB | ~50.000 |

Diese Modelle laufen auf CPUs mit AVX2 sehr schnell. Ein NPU/TPU lohnt sich erst bei:
- GroÃŸen Sprachmodellen (LLMs, >1 GB)
- Bildverarbeitung (CNNs)
- Echtzeit-Video-Analyse

**Fazit:** FÃ¼r Text-KI ist Intel N100 die beste Wahl!

---

## Option 2: Google Coral USB (Budget-Option)

### Vorteile
- **GÃ¼nstigste LÃ¶sung** (~60â‚¬)
- **Einfachste Installation** - USB 3.0 einstecken, fertig
- **Kein extra GerÃ¤t** - lÃ¤uft direkt am Werkstatt-Server
- **Kein Netzwerk-Setup** - keine mDNS-Discovery nÃ¶tig

### Architektur

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Bestehender Werkstatt-Server            â”‚
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Node.js     â”‚    â”‚ Google Coral USB        â”‚â”‚
â”‚  â”‚ Backend     â”‚â—„â”€â”€â–ºâ”‚ (4 TOPS Edge TPU)       â”‚â”‚
â”‚  â”‚             â”‚USB â”‚                         â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                 â”‚
â”‚  Alles auf einem GerÃ¤t - kein Netzwerk nÃ¶tig!  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Einkaufsliste

| Komponente | Preis |
|------------|-------|
| Google Coral USB Accelerator | ~60â‚¬ |
| **Gesamt** | **~60â‚¬** |

### Installation

```bash
# 1. Edge TPU Runtime installieren
echo "deb https://packages.cloud.google.com/apt coral-edgetpu-stable main" | \
  sudo tee /etc/apt/sources.list.d/coral-edgetpu.list
curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
sudo apt-get update
sudo apt-get install libedgetpu1-std

# 2. Python-Bibliothek
pip install pycoral

# 3. Coral USB einstecken (USB 3.0 Port!)
```

### Integration ins Backend

```python
# backend/src/services/coralAiService.py
from pycoral.utils.edgetpu import make_interpreter
from pycoral.adapters import common
import numpy as np

class CoralAiService:
    def __init__(self):
        self.zeit_model = None
        self.arbeiten_model = None

    def load_models(self):
        """LÃ¤dt TFLite-Modelle auf den Coral Edge TPU"""
        self.zeit_model = make_interpreter('models/zeit_schaetzung_edgetpu.tflite')
        self.zeit_model.allocate_tensors()

        self.arbeiten_model = make_interpreter('models/arbeiten_edgetpu.tflite')
        self.arbeiten_model.allocate_tensors()

        print("âœ… Coral Edge TPU Modelle geladen")

    def predict_zeit(self, arbeiten, fahrzeug=None):
        """ZeitschÃ¤tzung mit Edge TPU"""
        zeiten = []

        for arbeit in arbeiten:
            # Input vorbereiten
            input_data = self._text_to_input(arbeit)
            common.set_input(self.zeit_model, input_data)

            # Inferenz
            self.zeit_model.invoke()

            # Output lesen
            output = common.output_tensor(self.zeit_model, 0)
            minuten = float(output[0])

            zeiten.append({
                "arbeit": arbeit,
                "dauer_stunden": round(minuten / 60, 2)
            })

        return {
            "zeiten": zeiten,
            "gesamtdauer_stunden": sum(z["dauer_stunden"] for z in zeiten),
            "quelle": "coral-edge-tpu"
        }

    def _text_to_input(self, text):
        """Konvertiert Text zu Model-Input"""
        # Tokenisierung / Embedding
        pass
```

### Node.js Wrapper

```javascript
// backend/src/services/coralAiService.js
const { spawn } = require('child_process');
const path = require('path');

class CoralAiService {
  constructor() {
    this.pythonPath = process.env.PYTHON_PATH || 'python3';
    this.scriptPath = path.join(__dirname, 'coral_inference.py');
  }

  async estimateZeit(arbeiten, fahrzeug = '') {
    return new Promise((resolve, reject) => {
      const process = spawn(this.pythonPath, [
        this.scriptPath,
        'estimate_zeit',
        JSON.stringify({ arbeiten, fahrzeug })
      ]);

      let result = '';
      process.stdout.on('data', (data) => result += data);
      process.on('close', (code) => {
        if (code === 0) {
          resolve(JSON.parse(result));
        } else {
          reject(new Error('Coral inference failed'));
        }
      });
    });
  }
}

module.exports = new CoralAiService();
```

---

## Option 3: BeagleBone AI-64 (Industrie)

### Vorteile
- **Integrierter AI-Chip** (TI TDA4VM) - kein extra Modul
- **IndustriequalitÃ¤t** - robuster als Raspberry Pi
- **16GB eMMC** eingebaut - keine SD-Karte nÃ¶tig
- **Gutes Preis-Leistungs-VerhÃ¤ltnis**

### Architektur

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Lokales Netzwerk                         â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Werkstatt-Server   â”‚         â”‚    BeagleBone AI-64      â”‚ â”‚
â”‚  â”‚   (Backend + DB)     â”‚  HTTP   â”‚    (8 TOPS integriert)   â”‚ â”‚
â”‚  â”‚                      â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚                          â”‚ â”‚
â”‚  â”‚  - Node.js Backend   â”‚  REST   â”‚  - KI-Service (Python)   â”‚ â”‚
â”‚  â”‚  - SQLite DB         â”‚   API   â”‚  - TI Vision SDK         â”‚ â”‚
â”‚  â”‚  - Frontend          â”‚         â”‚  - mDNS Discovery        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Einkaufsliste

| Komponente | Preis |
|------------|-------|
| BeagleBone AI-64 (4GB, 16GB eMMC) | ~150â‚¬ |
| Netzteil 5V/3A USB-C | ~15â‚¬ |
| GehÃ¤use | ~20â‚¬ |
| **Gesamt** | **~185â‚¬** |

### Installation

```bash
# 1. Debian Image fÃ¼r BeagleBone AI-64 flashen
# Download: https://www.beagleboard.org/distros

# 2. TI Edge AI SDK installieren
sudo apt-get install ti-tidl-libs ti-vision-apps

# 3. Python-Umgebung
python3 -m venv ~/werkstatt-ki
source ~/werkstatt-ki/bin/activate
pip install fastapi uvicorn numpy onnxruntime-tidl
```

---

## Option 4: Raspberry Pi 5 + Hailo-8 (Overkill)

### Vorteile
- **HÃ¶chste Leistung** (26 TOPS)
- **GroÃŸe Community** - viel Support/Tutorials
- **Zukunftssicher** - auch fÃ¼r anspruchsvollere Modelle
- **Flexibel** - viele Erweiterungen mÃ¶glich

### Architektur

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Lokales Netzwerk                         â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Werkstatt-Server   â”‚         â”‚    Raspberry Pi 5        â”‚ â”‚
â”‚  â”‚   (Backend + DB)     â”‚  HTTP   â”‚    + Hailo-8 (26 TOPS)   â”‚ â”‚
â”‚  â”‚                      â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚                          â”‚ â”‚
â”‚  â”‚  - Node.js Backend   â”‚  REST   â”‚  - KI-Service (Python)   â”‚ â”‚
â”‚  â”‚  - SQLite DB         â”‚   API   â”‚  - Hailo Runtime         â”‚ â”‚
â”‚  â”‚  - Frontend          â”‚         â”‚  - mDNS Discovery        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Einkaufsliste

| Komponente | Preis |
|------------|-------|
| Raspberry Pi 5 B 8GB | ~90â‚¬ |
| Hailo-8 M.2 Modul | ~70â‚¬ |
| M.2 HAT fÃ¼r Pi 5 | ~15â‚¬ |
| NVMe SSD 256GB | ~30â‚¬ |
| Netzteil 27W USB-C | ~15â‚¬ |
| GehÃ¤use mit KÃ¼hlung | ~20â‚¬ |
| **Gesamt** | **~240â‚¬** |

### Installation

```bash
# 1. Raspberry Pi OS Lite (64-bit) flashen

# 2. Hailo Software Suite installieren
wget https://hailo.ai/downloads/hailo-8-software-suite.deb
sudo dpkg -i hailo-8-software-suite.deb
sudo apt-get install -f

# 3. PrÃ¼fen ob Hailo erkannt wird
hailortcli fw-control identify

# 4. Python-Umgebung
python3 -m venv ~/werkstatt-ki
source ~/werkstatt-ki/bin/activate
pip install fastapi uvicorn hailo-platform numpy
```

---

## Vergleich: Aktuelle KI vs. Hardware-KI

| Feature | Jetzt (Heuristik) | Mit AI-Hardware |
|---------|-------------------|-----------------|
| **ZeitschÃ¤tzung** | Keyword + Durchschnitte | ML mit Mustererkennung |
| **Arbeiten-VorschlÃ¤ge** | Wortlisten-Abgleich | NLP mit Kontext |
| **Teile-Erkennung** | Regelbasiert | ML-Klassifikation |
| **Genauigkeit** | ~70% | ~90%+ mÃ¶glich |
| **LernfÃ¤higkeit** | Nur Durchschnitte | Echtes Training |

### Konkretes Beispiel

```
Kunde sagt: "Bremsen quietschen beim Fahren, besonders bei KÃ¤lte"

Jetzt (Heuristik):
  â†’ Findet "Bremsen" als Keyword
  â†’ Vorschlag: "Bremsen" (generisch)

Mit AI-Hardware:
  â†’ Versteht Kontext und ZusammenhÃ¤nge
  â†’ Vorschlag: BremsbelÃ¤ge + Bremsscheiben prÃ¼fen
  â†’ Hinweis: Bei KÃ¤lte oft Rostbildung Ã¼ber Nacht
  â†’ ZeitschÃ¤tzung angepasst auf Fahrzeugtyp + km-Stand
```

---

## ErweiterungsmÃ¶glichkeiten

### 1. Intelligente ZeitschÃ¤tzung
- BerÃ¼cksichtigt Fahrzeugtyp, km-Stand, Saison
- Lernt aus historischen Daten der Werkstatt
- Erkennt Muster (z.B. "C3 + Bremsen = meist schneller")

### 2. Kontextbewusste Arbeiten-VorschlÃ¤ge
- Versteht natÃ¼rliche Sprache besser
- Erkennt zusammenhÃ¤ngende Probleme
- SchlÃ¤gt oft vergessene Zusatzarbeiten vor

### 3. Predictive Maintenance
- Analysiert Kundenhistorie
- Sagt anstehende Wartungen voraus
- "Zahnriemen bei km 120.000 fÃ¤llig" automatisch

### 4. Intelligente Tagesplanung
- Optimiert Terminverteilung
- BerÃ¼cksichtigt Mitarbeiter-Skills
- Minimiert Leerlauf und Wartezeiten

---

## Netzwerk-Discovery (fÃ¼r Standalone-Optionen)

FÃ¼r BeagleBone und Raspberry Pi: Automatische Erkennung via mDNS/Bonjour.

### KI-Device: Service ankÃ¼ndigen

```python
# discovery.py
from zeroconf import ServiceInfo, Zeroconf
import socket

def register_mdns_service(port=5000, device_type="coral"):
    hostname = socket.gethostname()
    local_ip = socket.gethostbyname(hostname + ".local")

    service_info = ServiceInfo(
        "_werkstatt-ki._tcp.local.",
        f"Werkstatt-KI-{hostname}._werkstatt-ki._tcp.local.",
        addresses=[socket.inet_aton(local_ip)],
        port=port,
        properties={
            'version': '1.0',
            'device': device_type,
            'capabilities': 'zeit-schaetzung,arbeiten-vorschlag,teile-erkennung'
        }
    )

    zeroconf = Zeroconf()
    zeroconf.register_service(service_info)
    print(f"âœ… KI-Service registriert: {local_ip}:{port}")
    return zeroconf
```

### Backend: Service finden

```javascript
// backend/src/services/kiDiscovery.js
const mdns = require('mdns-js');

class KIDiscoveryService {
  constructor() {
    this.discoveredServices = new Map();
  }

  startDiscovery() {
    const browser = mdns.createBrowser(mdns.tcp('werkstatt-ki'));

    browser.on('ready', () => {
      console.log('ğŸ” Suche nach KI-Services...');
      browser.discover();
    });

    browser.on('update', (data) => {
      this.discoveredServices.set(data.fullname, {
        host: data.host,
        port: data.port,
        addresses: data.addresses,
        properties: data.txt || {}
      });
      console.log(`âœ… KI-Service gefunden: ${data.host}:${data.port}`);
    });
  }

  getServiceUrl() {
    const service = [...this.discoveredServices.values()][0];
    if (service) {
      const ip = service.addresses?.[0] || service.host;
      return `http://${ip}:${service.port}`;
    }
    return null;
  }
}

module.exports = new KIDiscoveryService();
```

---

## KI-Service (FastAPI)

Gemeinsamer Service fÃ¼r alle Standalone-Optionen:

```python
# app/main.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional
import uvicorn

app = FastAPI(title="Werkstatt KI Service", version="1.0")

class ArbeitenRequest(BaseModel):
    beschreibung: str
    fahrzeug: Optional[str] = None

class ZeitRequest(BaseModel):
    arbeiten: List[str]
    fahrzeug: Optional[str] = None

# AI-Backend je nach Hardware
# from .coral_inference import CoralInference as AIBackend
# from .hailo_inference import HailoInference as AIBackend
# from .tidl_inference import TIDLInference as AIBackend

ai = AIBackend()

@app.get("/health")
async def health():
    return {"status": "ok", "device": ai.device_name}

@app.post("/api/suggest-arbeiten")
async def suggest_arbeiten(request: ArbeitenRequest):
    result = ai.predict_arbeiten(request.beschreibung, request.fahrzeug)
    return {"success": True, "data": result}

@app.post("/api/estimate-zeit")
async def estimate_zeit(request: ZeitRequest):
    result = ai.predict_zeit(request.arbeiten, request.fahrzeug)
    return {"success": True, "data": result}

@app.post("/api/teile-bedarf")
async def teile_bedarf(request: ArbeitenRequest):
    result = ai.predict_teile(request.beschreibung, request.fahrzeug)
    return {"success": True, "data": result}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=5000)
```

---

## Modell-Training

### Daten exportieren

```sql
-- Trainingsdaten aus Werkstatt-DB
SELECT
  arbeit,
  geschaetzte_zeit,
  tatsaechliche_zeit,
  fahrzeugtyp,
  kilometerstand
FROM termine
WHERE status = 'abgeschlossen'
  AND tatsaechliche_zeit > 0
  AND ki_training_exclude = 0;
```

### Modell trainieren

```python
# train_model.py
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sentence_transformers import SentenceTransformer
import tensorflow as tf

# Text-Embeddings erstellen
encoder = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# Daten laden
df = pd.read_csv('trainingsdaten.csv')
X = encoder.encode(df['arbeit'].tolist())
y = df['tatsaechliche_zeit'].values

# Modell trainieren
model = RandomForestRegressor(n_estimators=100)
model.fit(X, y)

# Zu TFLite konvertieren (fÃ¼r Coral)
# oder zu ONNX (fÃ¼r Hailo/BeagleBone)
```

### Modell konvertieren

```bash
# FÃ¼r Coral USB (TFLite + Edge TPU)
edgetpu_compiler zeit_model.tflite

# FÃ¼r Hailo-8 (ONNX â†’ HEF)
hailo_compiler --model zeit_model.onnx \
               --output zeit_model.hef \
               --hw-arch hailo8

# FÃ¼r BeagleBone (ONNX â†’ TIDL)
tidl_import_tool --model zeit_model.onnx
```

---

## Fallback-Strategie

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    KI-Anfrage                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Hardware-KI   â”‚ â—„â”€â”€ Coral / BeagleBone / Hailo
              â”‚ verfÃ¼gbar?    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
           Ja â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€ Nein
           â”‚                     â”‚
           â–¼                     â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ML-Modell   â”‚       â”‚ Lokale KI   â”‚ â—„â”€â”€ Heuristiken
    â”‚ (schnell)   â”‚       â”‚ (Fallback)  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Kostenvergleich

| Option | Einmalig | Strom/Jahr | Amortisation vs. ChatGPT |
|--------|----------|------------|--------------------------|
| **Intel N100 Mini-PC** | 130â‚¬ | ~5â‚¬ | ~1-2 Jahre |
| Coral USB | 60â‚¬ | ~1â‚¬ | < 1 Jahr |
| BeagleBone AI-64 | 185â‚¬ | ~5â‚¬ | ~2 Jahre |
| RPi 5 + Hailo-8 | 240â‚¬ | ~3â‚¬ | ~2-3 Jahre |
| ChatGPT API | 0â‚¬ | 50-100â‚¬ | - |

---

## Empfehlung

| Situation | Empfehlung |
|-----------|------------|
| **Beste Wahl** | ğŸ¥‡ Intel N100 Mini-PC (~130â‚¬) |
| **Nur USB-Erweiterung** | ğŸ¥ˆ Google Coral USB (~60â‚¬) |
| **Industrie/Robust** | ğŸ¥‰ BeagleBone AI-64 (~185â‚¬) |

**FÃ¼r die Werkstatt-KI empfohlen: Intel N100 Mini-PC**
- Beste Preis-Leistung fÃ¼r Text-KI
- 2x schneller als Raspberry Pi
- Einfache x86-Software-Installation
- Passiv gekÃ¼hlt, leise
- Keine Extra-Hardware (NPU/TPU) nÃ¶tig

---

## Implementierungsplan

| Phase | Aufgabe |
|-------|---------|
| 1 | Hardware beschaffen |
| 2 | Treiber/SDK installieren |
| 3 | Modelle trainieren (mit Werkstatt-Daten) |
| 4 | KI-Service implementieren |
| 5 | Backend-Integration |
| 6 | UI: KI-Modus "Hardware" in Einstellungen |
| 7 | Test & Optimierung |

---

## NÃ¤chste Schritte

- [ ] Hardware wÃ¤hlen und beschaffen
- [ ] Treiber installieren
- [ ] Trainingsdaten aus DB exportieren
- [ ] Modelle trainieren und konvertieren
- [ ] KI-Service implementieren
- [ ] Backend-Integration
- [ ] Testen und optimieren

---

## Ressourcen

### Intel N100 Mini-PCs
- [Intel N100 Specs](https://ark.intel.com/content/www/us/en/ark/products/231803/intel-processor-n100.html)
- [ONNX Runtime](https://onnxruntime.ai/) - Optimierte CPU-Inferenz
- [Sentence Transformers](https://www.sbert.net/) - MiniLM Text-Embeddings

### Google Coral
- [Coral Dokumentation](https://coral.ai/docs/)
- [Edge TPU Compiler](https://coral.ai/docs/edgetpu/compiler/)

### BeagleBone AI-64
- [BeagleBoard.org](https://www.beagleboard.org/boards/beaglebone-ai-64)
- [TI Edge AI SDK](https://www.ti.com/tool/PROCESSOR-SDK-LINUX-SK-TDA4VM)

### Raspberry Pi + Hailo
- [Hailo Developer Zone](https://hailo.ai/developer-zone/)
- [Hailo Model Zoo](https://github.com/hailo-ai/hailo_model_zoo)
- [Raspberry Pi 5 Docs](https://www.raspberrypi.com/documentation/)

### Allgemein
- [mDNS/Zeroconf Python](https://python-zeroconf.readthedocs.io/)
- [FastAPI](https://fastapi.tiangolo.com/)
