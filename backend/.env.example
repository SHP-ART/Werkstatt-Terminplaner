# Backend Environment Variables
# Kopiere diese Datei nach .env und passe die Werte an

# Server-Konfiguration
PORT=3001
NODE_ENV=development

# CORS-Konfiguration  
# Komma-separierte Liste erlaubter Origins
CORS_ORIGIN=http://localhost:3000,http://192.168.12.107:3000

# Datenbank
# Relativer Pfad zur SQLite-Datenbank
DB_PATH=./database/werkstatt.db
DATABASE_PATH=./database/werkstatt.db

# Backup-Konfiguration
BACKUP_PATH=./backups
BACKUP_RETENTION_DAYS=30

# Logging
LOG_LEVEL=info
LOG_FILE=./logs/backend.log

# Cache
CACHE_ENABLED=true
CACHE_TTL_MINUTES=5

# API-Rate-Limiting (optional)
RATE_LIMIT_ENABLED=false

# =============================================================================
# OpenAI / ChatGPT-Integration (Version 1.2.0)
# =============================================================================

# OpenAI API-Key (erforderlich f端r KI-Features)
# Hole deinen Key von: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-dein-api-key-hier

# OpenAI Modell (empfohlen: gpt-4o-mini f端r Kosteneffizienz)
OPENAI_MODEL=gpt-4o-mini

# Maximale Token pro Anfrage
OPENAI_MAX_TOKENS=1000

# Temperature (0.0-1.0, niedriger = deterministischer)
OPENAI_TEMPERATURE=0.3

# Monatliches Kosten-Limit in Euro (0 = kein Limit)
OPENAI_COST_LIMIT=50
RATE_LIMIT_WINDOW_MS=60000
RATE_LIMIT_MAX_REQUESTS=100

# =============================================================================
# Externer KI-Service (Lokale KI im Netzwerk)
# =============================================================================

# Basis-URL des lokalen KI-Services (Fallback, wenn Auto-Discovery nichts findet)
KI_EXTERNAL_URL=http://192.168.1.50:5000

# Timeout in Millisekunden fuer KI-Requests
KI_EXTERNAL_TIMEOUT_MS=4000

# Backend mDNS Discovery (fuer externe KI)
BACKEND_DISCOVERY_ENABLED=1

# =============================================================================
# Ollama (Lokales LLM auf Linux-Server)
# =============================================================================

# Basis-URL des Ollama-Servers (Standard: localhost, f端r Remote-Server IP anpassen)
# Beispiel Server im LAN: OLLAMA_BASE_URL=http://192.168.1.100:11434
OLLAMA_BASE_URL=http://localhost:11434

# Ollama-Modell (kleine schnelle Modelle empfohlen, z.B. llama3.2 oder mistral)
# Alle Modelle: ollama list
# Installieren:  ollama pull llama3.2
OLLAMA_MODEL=llama3.2

# Timeout in Millisekunden f端r Ollama-Requests (kleine Modelle brauchen ca. 5-15 Sek.)
OLLAMA_TIMEOUT_MS=15000

# Temperature (0.0-1.0, niedriger = deterministischer)
OLLAMA_TEMPERATURE=0.3
